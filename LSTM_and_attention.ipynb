{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "sequence"
      ],
      "metadata": {
        "id": "cl2FqSUFn7Uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.src.layers.rnn import Bidirectional\n",
        "# import libraries\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "sample = [10, 20, 30, 40, 50, 60, 70, 80, 90,100,110,120,130,140,150]\n",
        "X = [[10, 20, 30],[20, 30, 40],[30, 40, 50], [40, 50, 60], [50, 60, 70]]\n",
        "\n",
        "y = [40, 50, 60]\n",
        "\n",
        "# DEfine the function\n",
        "def split_sequence(sequence, n_steps):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequence)):\n",
        "        # find the end of this pattern\n",
        "        end_ix = i + n_steps\n",
        "        # check if we are beyond the sequence\n",
        "        if end_ix > len(sequence)-1:\n",
        "            break\n",
        "        # gather input and output parts of the pattern\n",
        "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return array(X), array(y)\n",
        "\n",
        "# define input sequence\n",
        "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90,100,110,120,130,140,150]\n",
        "# choose a number of time steps\n",
        "n_steps = 3\n",
        "# split into samples\n",
        "X, y = split_sequence(raw_seq, n_steps)\n",
        "# summarize the data\n",
        "for i in range(len(X)):\n",
        " print(X[i], y[i])\n",
        "\n",
        "\n",
        "# Deine model\n",
        "n_features = 1\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Reshape X to have the shape [samples, timesteps, features]\n",
        "X = X.reshape((X.shape[0], n_steps, n_features))\n",
        "\n",
        "# fit model\n",
        "model.fit(X, y, epochs=200, verbose = 0)\n",
        "x_input = array([70, 80, 90])\n",
        "x_input = x_input.reshape((1, n_steps, n_features))\n",
        "y = model.predict(x_input, verbose=0)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQbyUZZvpVrW",
        "outputId": "75310809-0f60-4148-eebe-2128d3cd82b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10 20 30] 40\n",
            "[20 30 40] 50\n",
            "[30 40 50] 60\n",
            "[40 50 60] 70\n",
            "[50 60 70] 80\n",
            "[60 70 80] 90\n",
            "[70 80 90] 100\n",
            "[ 80  90 100] 110\n",
            "[ 90 100 110] 120\n",
            "[100 110 120] 130\n",
            "[110 120 130] 140\n",
            "[120 130 140] 150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7a79b4c41480> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[100.109085]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention mechanism model"
      ],
      "metadata": {
        "id": "RgzUj3W_ktu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from numpy import random\n",
        "from numpy import dot\n",
        "from scipy.special import softmax\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# encoder representations of four different words\n",
        "Action = array([1, 0, 0])\n",
        "get = array([0, 1, 0])\n",
        "result= array([1, 1, 0])\n",
        "here = array([0, 0, 1])\n",
        "max_seq_len = 4\n",
        "\n",
        "# Generate positional encoding\n",
        "positional_encoding = np.zeros((max_seq_len, words.shape[1]))\n",
        "for pos in range(max_seq_len):\n",
        "    for i in range(words.shape[1]):\n",
        "        if i % 2 == 0:\n",
        "            positional_encoding[pos, i] = np.sin(pos / 10000 ** (i / words.shape[1]))\n",
        "        else:\n",
        "            positional_encoding[pos, i] = np.cos(pos / 10000 ** ((i - 1) / words.shape[1]))\n",
        "# Print the positional encoding values\n",
        "for pos in range(max_seq_len):\n",
        "    print(f\"Position {pos}: {positional_encoding[pos]}\")\n",
        "\n",
        "\n",
        "# stacking the word embeddings into a single array\n",
        "words = array([Action, get, result, here])\n",
        "\n",
        "# generating the weight matrices\n",
        "random.seed(42)\n",
        "W_Q = random.randint(3, size=(3, 3))\n",
        "W_K = random.randint(3, size=(3, 3))\n",
        "W_V = random.randint(3, size=(3, 3))\n",
        "\n",
        "# generating the queries, keys and values\n",
        "Q = words @ W_Q\n",
        "K = words @ W_K\n",
        "V = words @ W_V\n",
        "\n",
        "# scoring the query vectors against all key vectors\n",
        "scores = Q @ K.transpose()\n",
        "\n",
        "# computing the weights by a softmax operation\n",
        "weights = softmax(scores / K.shape[1] ** 0.5, axis=1)\n",
        "\n",
        "# computing the attention by a weighted sum of the value vectors\n",
        "attention = weights @ V\n",
        "\n",
        "print(attention)\n",
        "print(\"Data shape:\", words.shape)\n",
        "print(\"Positional Encoding shape:\", positional_encoding.shape)\n",
        "print(\"Weight Matrices shapes:\", W_Q.shape, W_K.shape, W_V.shape)\n",
        "print(\"Q, K, V shapes:\", Q.shape, K.shape, V.shape)\n",
        "print(\"Scores shape:\", scores.shape)\n",
        "print(\"Weights shape:\", weights.shape)\n"
      ],
      "metadata": {
        "id": "ptQpwW5cvdeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.special import softmax\n",
        "\n",
        "# Load your data from a CSV file\n",
        "data_set = pd.read_csv('/content/mdata.csv')\n",
        "text = \"My name is obasi yetunde I live in south korea\"\n",
        "tokens = text.split()\n",
        "print(tokens)\n",
        "\n",
        "# Create a DataFrame from the tokens\n",
        "data = pd.DataFrame([tokens])\n",
        "data = pd.DataFrame()\n",
        "\n",
        "\n",
        "unique_words = list(set(tokens))\n",
        "\n",
        "for word in unique_words:\n",
        "    data[word] = 0\n",
        "\n",
        "for word in tokens:\n",
        "    data.at[0, word] = 1\n",
        "\n",
        "# Define max_seq_len based on the number of columns (words) in your data\n",
        "max_seq_len = data.shape[1]\n",
        "positional_encoding = np.zeros((max_seq_len, data.shape[1]))\n",
        "\n",
        "# Generate positional encoding\n",
        "positional_encoding = np.zeros((max_seq_len, data.shape[1]))\n",
        "for pos in range(max_seq_len):\n",
        "    for i in range(data.shape[1]):\n",
        "        if i % 2 == 0:\n",
        "            positional_encoding[pos, i] = np.sin(pos / 10000 ** (i / data.shape[1]))\n",
        "        else:\n",
        "            positional_encoding[pos, i] = np.cos(pos / 10000 ** ((i - 1) / data.shape[1]))\n",
        "\n",
        "# Generating the weight matrices\n",
        "np.random.seed(42)\n",
        "W_Q = np.random.randint(10, size=(data.shape[1], data.shape[1]))\n",
        "W_K = np.random.randint(10, size=(data.shape[1], data.shape[1]))\n",
        "W_V = np.random.randint(10, size=(data.shape[1], data.shape[1]))\n",
        "\n",
        "# Generating the queries, keys, and values\n",
        "Q = data @ W_Q\n",
        "K = data @ W_K\n",
        "V = data @ W_V\n",
        "\n",
        "# Scoring the query vectors against all key vectors\n",
        "scores = Q @ K.transpose()\n",
        "\n",
        "# Computing the weights by a softmax operation\n",
        "weights = softmax(scores / K.shape[1] ** 0.5, axis=1)\n",
        "\n",
        "# Computing the attention by a weighted sum of the value vectors\n",
        "attention = weights @ V\n",
        "\n",
        "# Print the resulting attention\n",
        "\n",
        "\n",
        "print(attention)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiWdAo1QM7f_",
        "outputId": "739a5c06-b015-4e65-fce0-5ff59d044f4e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My', 'name', 'is', 'obasi', 'yetunde', 'I', 'live', 'in', 'south', 'korea']\n",
            "      0     1     2     3     4     5     6     7     8     9\n",
            "0  32.0  31.0  51.0  35.0  43.0  35.0  31.0  45.0  45.0  38.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention mechanism based on model"
      ],
      "metadata": {
        "id": "ikYDv2VLbT9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chart-studio"
      ],
      "metadata": {
        "id": "KiCIzonQbYpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chart-studio\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import time\n",
        "import string\n",
        "import os\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "# Set the file path here\n",
        "file_path = ('/content/eng-fra.txt')\n",
        "\n",
        "lines = open(file_path, encoding='UTF-8').read().strip().split('\\n')\n",
        "lines[5000:5010]\n",
        "\n",
        "\n",
        "print(\"total number of records: \",len(lines))\n",
        "\n",
        "exclude = set(string.punctuation) # Set of all special characters\n",
        "remove_digits = str.maketrans('', '', string.digits) # Set of all digit\n",
        "\n",
        "# function\n",
        "def preprocess_eng_sentence(sent):\n",
        "    '''Function to preprocess English sentence'''\n",
        "    sent = sent.lower() # lower casing\n",
        "    sent = re.sub(\"'\", '', sent) # remove the quotation marks if any\n",
        "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
        "    sent = sent.translate(remove_digits) # remove the digits\n",
        "    sent = sent.strip()\n",
        "    sent = re.sub(\" +\", \" \", sent) # remove extra spaces\n",
        "    sent = '<start> ' + sent + ' <end>' # add <start> and <end> tokens\n",
        "    return sent\n",
        "\n",
        "def preprocess_french_sentence(sentence):\n",
        "    # Your preprocessing code for French sentences\n",
        "    preprocessed_sentence = sentence.lower()  # Example: Convert to lowercase\n",
        "    preprocessed_sentence = '<start> ' + preprocessed_sentence + ' <end>'  # Add <start> and <end> tokens\n",
        "    return preprocessed_sentence\n",
        "\n",
        "\n",
        "sent_pairs = []\n",
        "for line in lines:\n",
        "    sent_pair = []\n",
        "    eng = line.rstrip().split('\\t')[0]\n",
        "    french = line.rstrip().split('\\t')[1]\n",
        "    eng = preprocess_eng_sentence(eng)\n",
        "    sent_pair.append(eng)\n",
        "    french = preprocess_french_sentence(french)  # Call the function and store the result\n",
        "    sent_pair.append(french)\n",
        "    sent_pairs.append(sent_pair)\n",
        "\n",
        "##\n",
        "class LanguageIndex():\n",
        "    def __init__(self, lang, add_start_end_tokens=False):\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "\n",
        "        self.create_index(add_start_end_tokens)\n",
        "\n",
        "    def create_index(self, add_start_end_tokens):\n",
        "        if add_start_end_tokens:\n",
        "            self.vocab.add(\"<start>\")\n",
        "            self.vocab.add(\"<end>\")\n",
        "\n",
        "        for phrase in self.lang:\n",
        "            self.vocab.update(phrase.split(' '))\n",
        "\n",
        "        self.vocab = sorted(self.vocab)\n",
        "\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1\n",
        "\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word\n",
        "\n",
        "inp_lang = LanguageIndex((en for en, ma in sent_pairs), add_start_end_tokens=True)\n",
        "targ_lang = LanguageIndex((ma for en, ma in sent_pairs), add_start_end_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "## Tokenization and padding\n",
        "def load_dataset(pairs, num_examples):\n",
        "    # pairs => already created cleaned input, output pairs\n",
        "\n",
        "    # index language using the class defined above\n",
        "    inp_lang = LanguageIndex(en for en, ma in pairs)\n",
        "    targ_lang = LanguageIndex(ma for en, ma in pairs)\n",
        "\n",
        "    # Vectorize the input and target languages\n",
        "\n",
        "    # English sentences\n",
        "    input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en, ma in pairs]\n",
        "\n",
        "    # Marathi sentences\n",
        "    target_tensor = [[targ_lang.word2idx[s] for s in ma.split(' ')] for en, ma in pairs]\n",
        "\n",
        "    # Calculate max_length of input and output tensor\n",
        "    # Here, we'll set those to the longest sentence in the dataset\n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "\n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor,\n",
        "                                                                 maxlen=max_length_inp,\n",
        "                                                                 padding='post')\n",
        "\n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor,\n",
        "                                                                  maxlen=max_length_tar, padding='post')\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n",
        "\n",
        "\n",
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(sent_pairs, len(lines))\n",
        "\n",
        "# Training and validation set 80-20\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1, random_state = 101)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)\n",
        "\n",
        "# parameters\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "def lstm(units):\n",
        "\n",
        "    return tf.keras.layers.LSTM(units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_activation='sigmoid',\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "# encoder\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = lstm(self.enc_units)\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state_h, state_c = self.lstm(x, initial_state=hidden)\n",
        "        return output, (state_h, state_c)\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = lstm(self.dec_units)  # Use 'gru' instead of 'lstm'\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # used for attention\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden[0], 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        output, state_h, state_c = self.lstm(x, initial_state=hidden)  # Initialize the LSTM with the hidden state\n",
        "\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, [state_h, state_c], attention_weights\n",
        "\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        #\n",
        "        return [tf.zeros((self.batch_sz, self.dec_units), dtype=tf.float32)] * 2\n",
        "\n",
        "\n",
        "# Add '<start>' token to the target language vocabulary\n",
        "\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# Define the loss function\n",
        "def loss_function(real, pred):\n",
        "      mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "      loss_ = tf.keras.losses.sparse_categorical_crossentropy(real, pred, from_logits=True)\n",
        "      mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "      loss_ *= mask\n",
        "      return tf.reduce_mean(loss_)\n",
        "\n",
        "## Training the model\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "\n",
        "            dec_hidden = enc_hidden\n",
        "\n",
        "\n",
        "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "\n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        variables = encoder.variables + decoder.variables\n",
        "\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every epoch\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO_FJAx4bbeq",
        "outputId": "029c95e7-f3c3-492c-9342-85716cd95497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chart-studio in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from chart-studio) (5.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from chart-studio) (2.31.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from chart-studio) (1.3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from chart-studio) (1.16.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->chart-studio) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly->chart-studio) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->chart-studio) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->chart-studio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->chart-studio) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->chart-studio) (2023.7.22)\n",
            "total number of records:  135842\n",
            "Epoch 1 Batch 0 Loss 1.4211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_jtkdRsXx-Rk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}